<div class="pg-simulation-container">
    <header class="simulation-header">
      <h1 class="title">Policy Gradient Methods Simulation</h1>
      <div class="subtitle">Mountain Car Environment</div>
      <div class="category-badge">Reinforcement Learning</div>
    </header>
  
    <div class="simulation-controls">
      <div class="control-buttons">
        <button class="control-btn" [class.active]="isPlaying" (click)="togglePlay()">
          <i class="icon">{{ isPlaying ? '‚è∏' : '‚ñ∂' }}</i>
          {{ isPlaying ? 'Pause' : 'Step' }}
        </button>
        <button class="control-btn" [class.active]="isAutoPlaying" (click)="toggleAutoPlay()">
          <i class="icon">üîÑ</i>
          {{ isAutoPlaying ? 'Stop Auto' : 'Auto Play' }}
        </button>
        <button class="control-btn" (click)="resetSimulation()">
          <i class="icon">‚Ü∫</i>
          Reset
        </button>
      </div>
      
      <div class="speed-controls">
        <span>Speed:</span>
        <button class="speed-btn" (click)="decreaseSpeed()" [disabled]="playSpeed <= 0.25">-</button>
        <span class="speed-display">{{ playSpeed }}x</span>
        <button class="speed-btn" (click)="increaseSpeed()" [disabled]="playSpeed >= 4">+</button>
      </div>
      
      <div class="simulation-info">
        <div class="info-item">
          <span class="info-label">Episode:</span>
          <span class="info-value">{{ episodeCount }}/{{ maxEpisodes }}</span>
        </div>
        <div class="info-item">
          <span class="info-label">Current Phase:</span>
          <span class="info-value phase-badge" [ngClass]="activeStep">
            {{ activeStep === 'environment' ? 'Environment Interaction' : 
               activeStep === 'policy' ? 'Policy Evaluation' :
               activeStep === 'gradient' ? 'Gradient Calculation' : 'Policy Update' }}
          </span>
        </div>
      </div>
    </div>
    
    <!-- Simulation Status Display -->
    <div class="simulation-status" *ngIf="simulationStatus">
      <div class="status-indicator" *ngIf="simulationInProgress">
        <div class="status-spinner"></div>
      </div>
      <div class="status-message">{{ simulationStatus }}</div>
    </div>
  
    <div class="simulation-grid">
      <!-- View Selector -->
      <div class="view-selector">
        <button class="view-btn" [class.active]="activeView === 'all'" (click)="setActiveView('all')">All Views</button>
        <button class="view-btn" [class.active]="activeView === 'environment'" (click)="setActiveView('environment')">Environment</button>
        <button class="view-btn" [class.active]="activeView === 'policy'" (click)="setActiveView('policy')">Policy</button>
        <button class="view-btn" [class.active]="activeView === 'rewards'" (click)="setActiveView('rewards')">Rewards</button>
        <button class="view-btn" [class.active]="activeView === 'landscape'" (click)="setActiveView('landscape')">Parameter Landscape</button>
      </div>
  
      <!-- 3D Environment Visualization -->
      <div class="grid-item environment-container" [class.full-width]="activeView === 'environment'" [class.hidden]="activeView !== 'environment' && activeView !== 'all'">
        <div class="section-header">
          <h2>Mountain Car Environment</h2>
          <div class="active-indicator" [class.active]="activeStep === 'environment'"></div>
        </div>
        <div class="simulation-canvas" #simulationContainer>
          <canvas></canvas>
        </div>
        <div class="info-panel environment-info" [ngStyle]="{'display': showExplanations ? 'block' : 'none'}">
          <h3>Environment Details</h3>
          <p>The Mountain Car problem is a classic reinforcement learning task where an underpowered car must drive up a steep hill. The car is too weak to directly climb the hill and must build momentum by driving back and forth.</p>
          <p>State: Position ({{ formatPosition(currentState.position) }}), Velocity ({{ formatVelocity(currentState.velocity) }})</p>
          <p>Goal: Reach the flag at position {{ goalPosition }}</p>
        </div>
      </div>
      
      <!-- Policy Visualization -->
      <div class="grid-item policy-container" [class.full-width]="activeView === 'policy'" [class.hidden]="activeView !== 'policy' && activeView !== 'all'">
        <div class="section-header">
          <h2>Policy Distribution</h2>
          <div class="active-indicator" [class.active]="activeStep === 'policy'"></div>
        </div>
        <div class="chart-container" #policyVisualization></div>
        <div class="info-panel policy-info" [ngStyle]="{'display': showExplanations ? 'block' : 'none'}">
          <h3>Gaussian Policy</h3>
          <p>Policy gradient methods use a parameterized policy œÄ<sub>Œ∏</sub> that maps states to action probabilities. For continuous action spaces, we typically use a Gaussian distribution.</p>
          <p>Mean force = Œ∏<sub>1</sub> √ó position + Œ∏<sub>2</sub> √ó velocity = {{ (policyParams.meanWeights[0] * currentState.position + policyParams.meanWeights[1] * currentState.velocity).toFixed(3) }}</p>
          <p>Policy parameters: Œ∏<sub>1</sub> = {{ policyParams.meanWeights[0].toFixed(3) }}, Œ∏<sub>2</sub> = {{ policyParams.meanWeights[1].toFixed(3) }}, œÉ = {{ policyParams.stdDev.toFixed(3) }}</p>
        </div>
      </div>
      
      <!-- Reward Chart -->
      <div class="grid-item reward-container" [class.full-width]="activeView === 'rewards'" [class.hidden]="activeView !== 'rewards' && activeView !== 'all'">
        <div class="section-header">
          <h2>Episode Rewards</h2>
        </div>
        <div class="chart-container" #rewardChart></div>
        <div class="info-panel rewards-info" [ngStyle]="{'display': showExplanations ? 'block' : 'none'}">
          <h3>Learning Progress</h3>
          <p>This chart shows the total reward per episode. Since the agent receives -1 reward per step, higher (less negative) values indicate faster goal achievement.</p>
          <p>The policy gradient objective function J(Œ∏) is to maximize the expected total reward.</p>
        </div>
      </div>
      
      <!-- Parameter Landscape -->
      <div class="grid-item parameter-container" [class.full-width]="activeView === 'landscape'" [class.hidden]="activeView !== 'landscape' && activeView !== 'all'">
        <div class="section-header">
          <h2>Parameter Landscape</h2>
          <div class="active-indicator" [class.active]="activeStep === 'gradient' || activeStep === 'update'"></div>
        </div>
        <div class="chart-container" #parameterLandscape></div>
        <div class="info-panel landscape-info" [ngStyle]="{'display': showExplanations ? 'block' : 'none'}">
          <h3>Policy Gradient Update</h3>
          <p>Policy gradient methods update parameters using gradient ascent: Œ∏ ‚Üê Œ∏ + Œ±‚àá<sub>Œ∏</sub>J(Œ∏)</p>
          <p>For a Gaussian policy with state features œÜ(s) = [position, velocity], the gradient is proportional to (a<sub>t</sub> - Œº(s<sub>t</sub>))œÜ(s<sub>t</sub>)R<sub>t</sub> where R<sub>t</sub> is the return.</p>
          <p>The orange point represents the current policy parameters, and the red arrow (during gradient phase) shows the direction of steepest ascent.</p>
        </div>
      </div>
    </div>
    
    <div class="explanation-toggle">
      <button class="toggle-btn" (click)="toggleExplanations()">
        {{ showExplanations ? 'Hide Explanations' : 'Show Explanations' }}
      </button>
      <span class="explanation-status" *ngIf="!showExplanations">Explanations are hidden</span>
    </div>
    
    <!-- Environment Details Section -->
    <div class="environment-details">
      <h2>Environment Details</h2>
      <div class="details-content">
        <p>The Mountain Car problem is a classic reinforcement learning task where an underpowered car must drive up a steep hill. The car is too weak to directly climb the hill and must build momentum by driving back and forth.</p>
        <p><strong>Current State:</strong> Position ({{ formatPosition(currentState.position) }}), Velocity ({{ formatVelocity(currentState.velocity) }})</p>
        <p><strong>Goal:</strong> Reach the flag at position {{ goalPosition }}</p>
        <p><strong>State Space:</strong> Position between {{ minPosition }} and {{ maxPosition }}, Velocity between -{{ maxVelocity }} and {{ maxVelocity }}</p>
        <p><strong>Action Space:</strong> Force between -1 and 1</p>
      </div>
    </div>
    
    <!-- Algorithm Details Section -->
    <div class="algorithm-details" [ngStyle]="{'display': showExplanations ? 'block' : 'none'}">
      <h2>Policy Gradient Methods Overview</h2>
      <div class="details-content">
        <p><strong>Purpose:</strong> Policy Gradient methods are reinforcement learning algorithms that directly optimize a parameterized policy to maximize expected rewards. They're especially effective for continuous action spaces.</p>
        
        <h3>Core Components</h3>
        <ol>
          <li><strong>Parameterized Policy (œÄ<sub>Œ∏</sub>):</strong> A function with tunable parameters Œ∏ that outputs action probabilities.</li>
          <li><strong>Objective Function (J(Œ∏)):</strong> The expected total reward obtained by following the policy.</li>
          <li><strong>Gradient Ascent:</strong> Updates parameters by moving in the direction of ‚àá<sub>Œ∏</sub>J(Œ∏) to maximize rewards.</li>
        </ol>
        
        <h3>REINFORCE Algorithm Steps</h3>
        <ol>
          <li><strong>Generate trajectories</strong> by executing the current policy</li>
          <li><strong>Calculate returns</strong> (discounted sum of rewards) for each step</li>
          <li><strong>Estimate policy gradient</strong> using ‚àá<sub>Œ∏</sub>log(œÄ<sub>Œ∏</sub>(a|s)) √ó G</li>
          <li><strong>Update policy parameters</strong> using gradient ascent</li>
        </ol>
        
        <h3>Key Advantages</h3>
        <ul>
          <li>Naturally handles continuous action spaces</li>
          <li>Can learn stochastic policies (important for exploration)</li>
          <li>Can be extended with baselines or critic networks (Actor-Critic methods)</li>
        </ul>
        
        <p>This simulation demonstrates the REINFORCE algorithm applied to the Mountain Car problem, showing how direct policy optimization can solve continuous control tasks.</p>
      </div>
    </div>
      <h2>Policy Gradient Methods Overview</h2>
      <div class="details-content">
        <p><strong>Purpose:</strong> Policy Gradient methods are reinforcement learning algorithms that directly optimize a parameterized policy to maximize expected rewards. They're especially effective for continuous action spaces.</p>
        
        <h3>Core Components</h3>
        <ol>
          <li><strong>Parameterized Policy (œÄ<sub>Œ∏</sub>):</strong> A function with tunable parameters Œ∏ that outputs action probabilities.</li>
          <li><strong>Objective Function (J(Œ∏)):</strong> The expected total reward obtained by following the policy.</li>
          <li><strong>Gradient Ascent:</strong> Updates parameters by moving in the direction of ‚àá<sub>Œ∏</sub>J(Œ∏) to maximize rewards.</li>
        </ol>
        
        <h3>REINFORCE Algorithm Steps</h3>
        <ol>
          <li><strong>Generate trajectories</strong> by executing the current policy</li>
          <li><strong>Calculate returns</strong> (discounted sum of rewards) for each step</li>
          <li><strong>Estimate policy gradient</strong> using ‚àá<sub>Œ∏</sub>log(œÄ<sub>Œ∏</sub>(a|s)) √ó G</li>
          <li><strong>Update policy parameters</strong> using gradient ascent</li>
        </ol>
        
        <h3>Key Advantages</h3>
        <ul>
          <li>Naturally handles continuous action spaces</li>
          <li>Can learn stochastic policies (important for exploration)</li>
          <li>Can be extended with baselines or critic networks (Actor-Critic methods)</li>
        </ul>
        
        <p>This simulation demonstrates the REINFORCE algorithm applied to the Mountain Car problem, showing how direct policy optimization can solve continuous control tasks.</p>
      </div>
    </div>
  
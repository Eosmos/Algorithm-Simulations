## Policy Gradient Methods: In-Depth Summary

**Purpose:**
Policy Gradient (PG) methods are a class of **reinforcement learning** algorithms that directly learn a **parameterized policy** which maps states to actions (or action probabilities), without necessarily needing to learn a value function first (though many advanced methods do use value functions). Their primary purpose is to optimize the policy parameters to maximize the **expected cumulative reward**. Key advantages and use cases include:
1.  **Handling Continuous Action Spaces:** Naturally applicable to problems where actions are continuous (e.g., robot joint torques, throttle control), which is difficult for purely value-based methods like Q-learning.
2.  **Learning Stochastic Policies:** Can directly learn policies that output probabilities over actions, which is optimal in partially observable environments or when exploration needs to be inherent in the policy.
3.  **Potentially Better Convergence:** Can sometimes offer smoother convergence properties compared to value-based methods, although they typically converge to a local optimum.

### Core Concept & Mechanism

1.  **Parameterized Policy (\(\pi_\theta\)):** The core idea is to represent the policy as a function with tunable parameters \(\theta\) (e.g., the weights of a neural network). This function \(\pi_\theta(a|s)\) outputs the probability of taking action \(a\) in state \(s\).
    *   For discrete actions: Output is a probability distribution over the finite set of actions.
    *   For continuous actions: Output represents parameters of a probability distribution (e.g., mean and standard deviation of a Gaussian), from which the action is sampled.
2.  **Objective Function (\(J(\theta)\)):** The goal is to find the parameters \(\theta\) that maximize an objective function \(J(\theta)\), typically defined as the expected total discounted reward obtained by following the policy \(\pi_\theta\):
    \[ J(\theta) = E_{\tau \sim \pi_\theta} [R(\tau)] = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r_{t+1} \right] \]
    where \(\tau = (s_0, a_0, r_1, s_1, \dots)\) is a trajectory (sequence of states, actions, rewards) generated by executing the policy \(\pi_\theta\), \(R(\tau)\) is the total discounted reward for the trajectory, \(\gamma\) is the discount factor, and \(r_{t+1}\) is the reward received after taking action \(a_t\) in state \(s_t\).
3.  **Gradient Ascent:** Policy Gradient methods update the policy parameters \(\theta\) by performing **gradient ascent** on the objective function \(J(\theta)\):
    \[ \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta) \]
    where \(\alpha\) is the learning rate and \(\nabla_\theta J(\theta)\) is the policy gradient.
4.  **Policy Gradient Theorem:** The key theoretical result that makes these methods feasible is the Policy Gradient Theorem. It provides a way to calculate or estimate the gradient \(\nabla_\theta J(\theta)\) without needing to know the environment's dynamics (transition probabilities). A common form of the theorem is:
    \[ \nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \, \Psi_t \right] \]
    *   \(\nabla_\theta \log \pi_\theta(a_t|s_t)\): This term is the **score function**. It indicates how sensitive the log-probability of taking action \(a_t\) in state \(s_t\) is to changes in the parameters \(\theta\). Crucially, this depends only on the policy itself and can be calculated.
    *   \(\Psi_t\): This term is a measure of how "good" the action \(a_t\) taken at time step \(t\) was. It weights the score function. Different algorithms use different choices for \(\Psi_t\):
        *   **REINFORCE Algorithm:** Uses the total discounted return from time step \(t\) onwards: \(\Psi_t = G_t = \sum_{k=t}^{T} \gamma^{k-t} r_{k+1}\). Actions leading to higher future rewards are reinforced more strongly.
        *   **Baselines:** To reduce variance, a state-dependent baseline \(b(s_t)\) (often an estimate of the state value function \(V(s_t)\)) is subtracted: \(\Psi_t = G_t - b(s_t)\). This centers the rewards, reinforcing actions better than average for that state.
        *   **Actor-Critic Methods:** Use an estimate of the Advantage function \(A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\) for \(\Psi_t\). The 'Actor' is the policy \(\pi_\theta\), and the 'Critic' is a separate function approximator (e.g., another neural network) that estimates the value function (Q or V) to provide a lower-variance signal for \(\Psi_t\).

### Algorithm (Step-by-Step Process using REINFORCE)

1.  **Initialization:** Initialize the policy parameters \(\theta\) (e.g., random weights for a neural network policy). Choose a learning rate \(\alpha\).
2.  **Training Loop:** Repeat for a number of iterations:
    *   **Generate Trajectories:** Execute the current policy \(\pi_\theta\) in the environment to collect one or more complete trajectories \(\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \dots, s_T, a_T, r_{T+1})\).
    *   **Calculate Returns:** For each trajectory collected, and for each time step \(t\) (from 0 to T), calculate the discounted return-to-go:
        \[ G_t = \sum_{k=t}^{T} \gamma^{k-t} r_{k+1} \]
    *   **Estimate Policy Gradient:** Compute the gradient estimate \(\hat{g}\) by averaging over the collected trajectories and time steps:
        \[ \hat{g} = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T_i} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \, G_{i,t} \]
        where \(N\) is the number of trajectories, \(T_i\) is the length of trajectory \(i\), and \((s_{i,t}, a_{i,t}, G_{i,t})\) are the state, action, and return from trajectory \(i\) at time \(t\).
        *(Note: Calculating \(\nabla_\theta \log \pi_\theta(a|s)\) involves automatic differentiation if using frameworks like TensorFlow or PyTorch).*
    *   **Update Parameters:** Perform a gradient ascent step:
        \[ \theta \leftarrow \theta + \alpha \hat{g} \]
    *   Discard the collected trajectories (since REINFORCE is on-policy) and repeat the loop with the updated policy.

### Assumptions and Key Details

*   **High Variance:** The gradient estimates in basic PG methods (like REINFORCE) using \(G_t\) often have high variance because the return depends on many random actions and state transitions. This can make learning slow and unstable. Techniques like baselines and actor-critic methods are crucial for practical applications.
*   **On-Policy:** Most simple PG algorithms are **on-policy**, meaning the trajectories used to compute the gradient must be generated by the *current* version of the policy \(\pi_\theta\). This can be sample-inefficient as data is often discarded after one update. (Advanced methods like PPO offer some improvements).
*   **Local Optima:** Gradient ascent guarantees convergence only to a local optimum of the objective function \(J(\theta)\).
*   **Sensitivity to Hyperparameters:** Performance is sensitive to the learning rate \(\alpha\) and potentially other parameters like the neural network architecture.
*   **Implementation:** Calculating \(\nabla_\theta \log \pi_\theta(a|s)\) depends on the policy representation. For a softmax policy (discrete actions) or Gaussian policy (continuous actions), this term is analytically tractable.

### Simulation Ideas for Visualization

1.  **Continuous Control Task (e.g., Mountain Car Continuous, Pendulum):**
    *   Show the agent attempting the task (e.g., car trying to reach the flag, pendulum trying to stay upright).
    *   Visualize the learned policy: Plot the output distribution (e.g., Gaussian) for the action (force/torque) given the current state. Show how this distribution changes over training (mean shifts towards optimal actions, variance might decrease).
    *   Plot the episode reward over training iterations, showing the agent improving.

2.  **Policy Parameter Landscape (Simplified):**
    *   Imagine a 2D plot representing the objective function \(J(\theta)\) surface for two policy parameters \(\theta_1, \theta_2\).
    *   Show a point representing the current \(\theta\) value.
    *   After collecting trajectories and estimating the gradient \(\nabla_\theta J(\theta)\), visualize this gradient as an arrow pointing uphill on the landscape.
    *   Animate the point \(\theta\) moving in the direction of the arrow (gradient ascent step). Show convergence towards a peak (local optimum).

3.  **Trajectory Weighting:**
    *   Show a completed trajectory with its associated rewards.
    *   Calculate the returns \(G_t\) for each step \(t\).
    *   Highlight the \(\log \pi_\theta(a_t|s_t)\) term for each step.
    *   Visually emphasize that the updates \(\nabla_\theta \log \pi_\theta(a_t|s_t) \, G_t\) are larger for steps that were part of high-reward trajectories (high \(G_t\)) and smaller (or negative, pushing away) for steps in low-reward trajectories. If using a baseline, show how the weighting \((G_t - b(s_t))\) focuses on whether the action was better or worse *than average* for that state.

4.  **Discrete Action Probabilities (e.g., CartPole):**
    *   Show the CartPole balancing.
    *   Visualize the policy output as probabilities for "push left" vs. "push right" given the current state (pole angle, velocities). Display these as changing bar heights or pie chart segments.
    *   Show how these probabilities shift during training to favor the action that leads to longer balancing times (higher rewards).

### Research Paper

*   **Foundational Paper (REINFORCE):**
    *   **Williams, R. J. (1992). "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning".** *Machine Learning*. 8(3–4): 229–256.
*   **Actor-Critic & Related Concepts:** While REINFORCE is fundamental, much of modern PG research builds on Actor-Critic ideas and variance reduction, with significant contributions from:
    *   **Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. (1999). "Policy Gradient Methods for Reinforcement Learning with Function Approximation".** *Advances in Neural Information Processing Systems (NIPS)*. 12.

These simulations can help illustrate how policies are directly optimized, the role of trajectory rewards in guiding updates, the concept of gradient ascent on the policy parameters, and the specific challenges (like variance) and advantages (like handling continuous actions) of these methods.
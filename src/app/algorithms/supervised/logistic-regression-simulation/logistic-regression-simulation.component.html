<div class="logistic-regression-container">
  <!-- Header Section -->
  <div class="header-section">
    <div class="algorithm-info">
      <h1>Logistic Regression</h1>
      <div class="category-badge">Supervised Learning</div>
      <p class="algorithm-description">
        A classification algorithm that estimates the probability of an instance belonging to a particular class.
      </p>
    </div>
    <div class="controls">
      <div class="play-controls">
        <button class="control-btn" [class.active]="isPlaying" (click)="isPlaying ? stopSimulation() : playSimulation()">
          <i class="fa" [class.fa-pause]="isPlaying" [class.fa-play]="!isPlaying"></i>
          <span [textContent]="isPlaying ? 'Pause' : 'Play'"></span>
        </button>
        <button class="control-btn" (click)="resetSimulation()">
          <i class="fa fa-refresh"></i>
          Reset
        </button>
        <button class="control-btn" (click)="runOneIteration()">
          <i class="fa fa-step-forward"></i>
          Step
        </button>
        <button class="control-btn" (click)="runMultipleIterations(10)">
          <i class="fa fa-fast-forward"></i>
          +10 Steps
        </button>
        <button class="control-btn" (click)="runAllIterations()">
          <i class="fa fa-forward"></i>
          Complete
        </button>
      </div>
      <div class="simulation-controls">
        <div class="control-group">
          <label for="learning-rate">Learning Rate:</label>
          <select id="learning-rate" (change)="setLearningRate($any($event.target).value)">
            <option value="0.01">0.01</option>
            <option value="0.05">0.05</option>
            <option value="0.1" selected>0.1</option>
            <option value="0.2">0.2</option>
            <option value="0.5">0.5</option>
          </select>
        </div>
        <div class="control-group">
          <label for="play-speed">Speed:</label>
          <select id="play-speed" (change)="setSpeed($any($event.target).value)">
            <option value="3000">Slow</option>
            <option value="2000" selected>Normal</option>
            <option value="1000">Fast</option>
            <option value="500">Very Fast</option>
          </select>
        </div>
        <button class="control-btn generate-btn" (click)="generateNewData()">
          <i class="fa fa-random"></i>
          New Data
        </button>
      </div>
    </div>
  </div>

  <!-- Navigation Tabs -->
  <div class="nav-tabs">
    <div class="tab" [class.active]="activePage === 1" (click)="changePage(1)">
      <i class="fa fa-eye"></i>
      Visualization
    </div>
    <div class="tab" [class.active]="activePage === 2" (click)="changePage(2)">
      <i class="fa fa-book"></i>
      Concept
    </div>
    <div class="tab" [class.active]="activePage === 3" (click)="changePage(3)">
      <i class="fa fa-code"></i>
      Implementation
    </div>
    <div class="tab" [class.active]="activePage === 4" (click)="changePage(4)">
      <i class="fa fa-graduation-cap"></i>
      Research
    </div>
  </div>

  <!-- Main Content Area -->
  <div class="content-area">
    <!-- Visualization Page -->
    <div class="page" [hidden]="activePage !== 1">
      <div class="visualization-grid">
        <div class="viz-container sigmoid-container">
          <div #sigmoidCanvas class="visualization-canvas"></div>
        </div>
        <div class="viz-container decision-boundary-container">
          <div #decisionBoundaryCanvas class="visualization-canvas"></div>
        </div>
        <div class="viz-container cost-function-container">
          <div #costFunctionCanvas class="visualization-canvas"></div>
        </div>
        <div class="viz-container heatmap-container">
          <div #heatmapCanvas class="visualization-canvas"></div>
        </div>
      </div>
      <div class="current-parameters">
        <h3>Current Model Parameters</h3>
        <div class="parameter-list">
          <div class="parameter">
            <span class="parameter-name">β₀ (Bias):</span>
            <span class="parameter-value" [textContent]="beta0.toFixed(4)"></span>
          </div>
          <div class="parameter">
            <span class="parameter-name">β₁ (Weight 1):</span>
            <span class="parameter-value" [textContent]="beta1.toFixed(4)"></span>
          </div>
          <div class="parameter">
            <span class="parameter-name">β₂ (Weight 2):</span>
            <span class="parameter-value" [textContent]="beta2.toFixed(4)"></span>
          </div>
          <div class="parameter">
            <span class="parameter-name">Iteration:</span>
            <span class="parameter-value">
              <span [textContent]="currentIteration"></span> / <span [textContent]="iterations"></span>
            </span>
          </div>
          <div class="parameter">
            <span class="parameter-name">Cost:</span>
            <span class="parameter-value" [textContent]="getFormattedCurrentCost()"></span>
          </div>
        </div>
      </div>
    </div>

    <!-- Concept Page -->
    <div class="page" [hidden]="activePage !== 2">
      <div class="content-section">
        <h2>Logistic Regression: Overview</h2>
        <div class="concept-explanation">
          <p>
            Logistic Regression is a fundamental <strong>supervised learning</strong> algorithm primarily used for <strong>binary classification</strong> tasks, 
            where the goal is to predict one of two possible outcomes (e.g., Yes/No, Spam/Not Spam, True/False, 0/1). Despite its name, 
            it's a classification algorithm, not a regression one (though it predicts probabilities, which are continuous).
          </p>
          
          <div class="key-points">
            <div class="point">
              <h3>Core Concept & Mechanism</h3>
              <ol>
                <li>
                  <strong>Linear Combination:</strong> Like Linear Regression, it starts by calculating a weighted sum of the input features (x<sub>i</sub>) 
                  plus a bias term (β<sub>0</sub>). This linear combination is often called the <strong>logit</strong> or log-odds:
                  <div class="equation">z = β₀ + β₁x₁ + β₂x₂ + ⋯ + βₙxₙ</div>
                  Here, β<sub>i</sub> are the model parameters (coefficients) learned during training.
                </li>
                <li>
                  <strong>Sigmoid (Logistic) Function:</strong> Instead of using z directly as the output, Logistic Regression applies the 
                  <strong>sigmoid function</strong> to squash the result into the range [0, 1]:
                  <div class="equation">P(y=1|x) = σ(z) = 1 / (1 + e^(-z))</div>
                  The output P(y=1|x) represents the estimated <strong>probability</strong> that the given input instance x belongs to the positive class (class 1).
                </li>
                <li>
                  <strong>Decision Boundary:</strong> To make a final classification, a threshold (typically 0.5) is applied to the predicted probability:
                  <ul>
                    <li>If P(y=1|x) ≥ 0.5, predict class 1.</li>
                    <li>If P(y=1|x) < 0.5, predict class 0.</li>
                  </ul>
                  The decision boundary occurs where P(y=1|x) = 0.5, which corresponds to z = 0.
                </li>
              </ol>
            </div>
          </div>
        </div>
      </div>
      
      <div class="content-section">
        <h2>Training Process</h2>
        <div class="training-process">
          <ol class="process-steps">
            <li>
              <h3>Data Preparation</h3>
              <p>Gather labeled training data with input features and corresponding binary class labels (0 or 1). 
                Preprocess features through scaling, normalization, and handling missing values.</p>
            </li>
            <li>
              <h3>Initialize Parameters</h3>
              <p>Start with initial values for the coefficients β (often zeros or small random numbers).</p>
            </li>
            <li>
              <h3>Define Cost Function</h3>
              <p>The goal is to find the β values that minimize the error between predicted probabilities and actual labels. 
                The standard cost function for Logistic Regression is <strong>Cross-Entropy Loss</strong> (Log Loss):</p>
              <div class="equation">J(β) = -1/m ∑[y^(i) log(h_β(x^(i))) + (1 - y^(i)) log(1 - h_β(x^(i)))]</div>
              <p>This function penalizes confident wrong predictions heavily.</p>
            </li>
            <li>
              <h3>Optimization (Gradient Descent)</h3>
              <p>Use an optimization algorithm like <strong>Gradient Descent</strong> to iteratively update the parameters β in the direction 
                that minimizes the cost function. The process repeats until the cost converges to a minimum.</p>
            </li>
            <li>
              <h3>Model Ready</h3>
              <p>Once the optimal β parameters are found, the model is trained and ready for prediction.</p>
            </li>
          </ol>
        </div>
      </div>
      
      <div class="content-section">
        <h2>Key Assumptions & Characteristics</h2>
        <div class="assumption-list">
          <div class="assumption">
            <i class="fa fa-check-circle"></i>
            <p>Assumes a <strong>linear relationship</strong> between the features and the log-odds of the outcome.</p>
          </div>
          <div class="assumption">
            <i class="fa fa-check-circle"></i>
            <p>Features should be independent, though it often performs well even when features are correlated.</p>
          </div>
          <div class="assumption">
            <i class="fa fa-check-circle"></i>
            <p>Highly <strong>interpretable</strong> - the sign and magnitude of coefficients can indicate direction and strength of association.</p>
          </div>
          <div class="assumption">
            <i class="fa fa-check-circle"></i>
            <p>Can be sensitive to <strong>outliers</strong> in the training data.</p>
          </div>
          <div class="assumption">
            <i class="fa fa-check-circle"></i>
            <p>The decision boundary created is <strong>linear</strong> in the feature space.</p>
          </div>
        </div>
      </div>
      
      <div class="content-section">
        <h2>Common Use Cases</h2>
        <div class="use-cases-grid">
          <div class="use-case">
            <div class="use-case-icon">
              <i class="fa fa-heartbeat"></i>
            </div>
            <h3>Medical Diagnosis</h3>
            <p>Predicting disease presence based on patient symptoms and test results.</p>
          </div>
          <div class="use-case">
            <div class="use-case-icon">
              <i class="fa fa-credit-card"></i>
            </div>
            <h3>Credit Scoring</h3>
            <p>Determining credit approval or fraud detection based on financial history.</p>
          </div>
          <div class="use-case">
            <div class="use-case-icon">
              <i class="fa fa-envelope"></i>
            </div>
            <h3>Spam Detection</h3>
            <p>Classifying emails as spam or legitimate based on content and metadata.</p>
          </div>
          <div class="use-case">
            <div class="use-case-icon">
              <i class="fa fa-user"></i>
            </div>
            <h3>Customer Behavior</h3>
            <p>Predicting whether a customer will purchase a product or service.</p>
          </div>
        </div>
      </div>
    </div>

    <!-- Implementation Page -->
    <div class="page" [hidden]="activePage !== 3">
      <div class="implementation-section">
        <h2>Logistic Regression Implementation</h2>
        <p>Below are implementations of Logistic Regression in popular programming languages and frameworks.</p>
      </div>

      <div class="implementation-section">
        <h2>Python Implementation</h2>
        <div class="code-container">
          <pre class="code-block">{{ pythonImplementation }}</pre>
        </div>
      </div>

      <div class="implementation-section">
        <h2>JavaScript Implementation</h2>
        <div class="code-container">
          <pre class="code-block">{{ jsImplementation }}</pre>
        </div>
      </div>

      <div class="implementation-section">
        <h2>R Implementation</h2>
        <div class="code-container">
          <pre class="code-block">{{ rImplementation }}</pre>
        </div>
      </div>

      <div class="implementation-section">
        <h2>Common Variations</h2>
        <div class="variations">
          <div class="variation">
            <h3>Multinomial Logistic Regression</h3>
            <p>Extension of the binary case to handle multiple classes using softmax function instead of sigmoid.</p>
            <div class="equation">P(y=k|x) = e^(β₀ᵏ + β₁ᵏx₁ + ...) / ∑ e^(β₀ʲ + β₁ʲx₁ + ...)</div>
          </div>
          <div class="variation">
            <h3>Regularized Logistic Regression</h3>
            <p>Adds penalty terms to prevent overfitting:</p>
            <div class="equation">L1 (Lasso): J(β) + λ∑|βᵢ|</div>
            <div class="equation">L2 (Ridge): J(β) + λ∑βᵢ²</div>
          </div>
          <div class="variation">
            <h3>Bayesian Logistic Regression</h3>
            <p>Treats model parameters as random variables with prior distributions, providing uncertainty estimates.</p>
          </div>
        </div>
      </div>

      <div class="implementation-section">
        <h2>Popular Tools & Libraries</h2>
        <div class="tools-grid">
          <div class="tool">
            <h3>Scikit-Learn (Python)</h3>
            <div class="code-snippet">
              <pre>
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)</pre>
            </div>
          </div>
          <div class="tool">
            <h3>TensorFlow/Keras (Python)</h3>
            <div class="code-snippet">
              <pre>
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
  Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10)</pre>
            </div>
          </div>
          <div class="tool">
            <h3>StatsModels (Python)</h3>
            <div class="code-snippet">
              <pre>
import statsmodels.api as sm
X_with_const = sm.add_constant(X)
model = sm.Logit(y, X_with_const)
result = model.fit()
print(result.summary())</pre>
            </div>
          </div>
          <div class="tool">
            <h3>GLM (R)</h3>
            <div class="code-snippet">
              <pre>
# Fit logistic regression in R
model <- glm(formula = y ~ x1 + x2,
             family = binomial(link = "logit"),
             data = training_data)
summary(model)
predictions <- predict(model, newdata=test_data, type="response")</pre>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Research Page -->
    <div class="page" [hidden]="activePage !== 4">
      <div class="research-section">
        <h2>Historical Development</h2>
        <div class="timeline">
          @for (event of timelineEvents; track event.year) {
            <div class="timeline-item">
              <div class="timeline-date">{{ event.year }}</div>
              <div class="timeline-content">
                <h3>{{ event.title }}</h3>
                <p>{{ event.content }}</p>
              </div>
            </div>
          }
        </div>
      </div>

      <div class="research-section">
        <h2>Key Research Papers</h2>
        <div class="papers-list">
          @for (paper of researchPapers; track paper.title) {
            <div class="paper">
              <h3>{{ paper.title }}</h3>
              <div class="paper-details">
                <div><strong>Authors:</strong> {{ paper.authors }}</div>
                <div><strong>Year:</strong> {{ paper.year }}</div>
                <div><strong>Journal:</strong> {{ paper.journal }}</div>
                <div><strong>DOI:</strong> <a [href]="paper.doi" target="_blank">{{ paper.doi }}</a></div>
              </div>
            </div>
          }
        </div>
      </div>

      <div class="research-section">
        <h2>Advantages & Limitations</h2>
        <div class="pros-cons">
          <div class="pros">
            <h3>Advantages</h3>
            <ul>
              <li>Highly interpretable - coefficients directly indicate feature importance and direction</li>
              <li>Outputs well-calibrated probabilities (unlike SVM)</li>
              <li>Efficient training even on large datasets</li>
              <li>Minimal hyperparameter tuning required</li>
              <li>Resistant to overfitting when regularized</li>
              <li>Works well even when data is not linearly separable</li>
            </ul>
          </div>
          <div class="cons">
            <h3>Limitations</h3>
            <ul>
              <li>Can only model linear decision boundaries without feature engineering</li>
              <li>May underperform with highly imbalanced datasets</li>
              <li>Assumes independence of features (though often works regardless)</li>
              <li>Sensitive to outliers in the training data</li>
              <li>May struggle with high-dimensional feature spaces</li>
              <li>Can suffer from multicollinearity issues</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="research-section">
        <h2>Related Algorithms</h2>
        <div class="related-algorithms">
          <div class="algorithm">
            <h3>Support Vector Machines</h3>
            <p>Focuses on finding the optimal hyperplane that maximizes the margin between classes, rather than modeling probabilities.</p>
          </div>
          <div class="algorithm">
            <h3>Decision Trees</h3>
            <p>Creates non-linear decision boundaries through recursive partitioning of the feature space into regions.</p>
          </div>
          <div class="algorithm">
            <h3>Naive Bayes</h3>
            <p>Generative model that applies Bayes' theorem with strong feature independence assumptions.</p>
          </div>
          <div class="algorithm">
            <h3>Neural Networks</h3>
            <p>A single-layer neural network with sigmoid activation is equivalent to logistic regression, but multi-layer networks can model non-linear boundaries.</p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Footer -->
  <div class="footer">
    <div class="pagination">
      <button class="pagination-btn" [disabled]="activePage === 1" (click)="prevPage()">
        <i class="fa fa-chevron-left"></i>
        Previous
      </button>
      <div class="page-indicators">
        @for (i of [1,2,3,4]; track i) {
          <span class="page-dot" [class.active]="activePage === i" (click)="changePage(i)"></span>
        }
      </div>
      <button class="pagination-btn" [disabled]="activePage === totalPages" (click)="nextPage()">
        Next
        <i class="fa fa-chevron-right"></i>
      </button>
    </div>
    <div class="copyright">
      © 2025 AI Algorithm Explorer
    </div>
  </div>
</div>
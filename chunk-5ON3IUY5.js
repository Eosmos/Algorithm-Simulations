import{a as Pt}from"./chunk-ZW3SWSXV.js";import{I as gt,L as Z,N as F,R as ht,S as vt,X as xt,Z as ft,k as dt,q as B,t as pt,u as mt,v as ut}from"./chunk-IMA2JTAA.js";import{c as J,i as A,j as G,k as H,r as w,s as st,u as ct,w as D}from"./chunk-SKIYKJGI.js";import{h as ot,i as rt,j as lt}from"./chunk-7EKIJKTV.js";import{Cb as at,Da as et,Ia as Q,Oa as C,Qa as E,X as x,Xa as n,Y as f,Ya as e,Z as z,Za as _,_ as R,ab as nt,db as P,eb as $,fa as K,gb as k,hb as j,ib as L,kb as t,lb as O,mb as I,nb as it,ra as X,ta as tt,ua as s,vb as V,za as W}from"./chunk-CS32D7W3.js";var Mt=["pendulumCanvas"],bt=["parameterLandscape"],Ot=["trajectoryVisualization"],yt=["policyOutput"],q=(u,a)=>({visible:u,hidden:a});function St(u,a){if(u&1&&(n(0,"p",79),t(1),e()),u&2){let o=$().$implicit;s(),O(o.volume)}}function Et(u,a){if(u&1&&(n(0,"p",80),t(1),e()),u&2){let o=$().$implicit;s(),O(o.pages)}}function wt(u,a){if(u&1&&(n(0,"div",67)(1,"div",68),z(),n(2,"svg",69),_(3,"path",70)(4,"path",71)(5,"path",72),e()(),R(),n(6,"div",73)(7,"h3"),t(8),e(),n(9,"p",74),t(10),e(),n(11,"p",75),t(12),e(),Q(13,St,2,1,"p",76)(14,Et,2,1,"p",77),n(15,"a",78),t(16,"View Paper"),e()()()),u&2){let o=a.$implicit;s(8),O(o.title),s(2),O(o.author),s(2),it("",o.journal," (",o.year,")"),s(),C("ngIf",o.volume),s(),C("ngIf",o.pages),s(),C("href",o.url,X)}}var Ct=class u{constructor(a,o){this.ngZone=a;this.changeDetector=o}pendulumCanvasRef;parameterLandscapeRef;trajectoryVisualizationRef;policyOutputRef;activeTab="overview";isSimulationRunning=!1;isAutoPlay=!1;learningRate=.01;discountFactor=.99;episodeCount=0;totalReward=0;currentStep=0;pendulumInitialized=!1;visualizationsInitialized=!1;papers=[{title:"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",author:"Williams, R. J.",year:1992,journal:"Machine Learning",volume:"8(3\u20134)",pages:"229\u2013256",url:"https://link.springer.com/article/10.1007/BF00992696"},{title:"Policy Gradient Methods for Reinforcement Learning with Function Approximation",author:"Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y.",year:1999,journal:"Advances in Neural Information Processing Systems (NIPS)",volume:"12",url:"https://proceedings.neurips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html"}];scene;camera;renderer;controls;pendulum;policy=new N(2,1);animationId=null;trajectories=[];currentTrajectory=null;paramSvg;trajectorySvg;policySvg;parameterHistory=[];ngOnInit(){this.resetSimulation()}ngAfterViewInit(){this.changeDetector.detectChanges(),this.activeTab==="simulation"&&setTimeout(()=>{this.initializeVisualizationsIfNeeded()},500)}ngOnDestroy(){this.animationId!==null&&cancelAnimationFrame(this.animationId),this.renderer&&(this.renderer.dispose(),this.renderer.forceContextLoss())}selectTab(a){this.activeTab=a,a==="simulation"&&setTimeout(()=>{this.initializeVisualizationsIfNeeded()},100)}initializeVisualizationsIfNeeded(){try{!this.pendulumInitialized&&this.pendulumCanvasRef?.nativeElement&&(this.initPendulumScene(),this.pendulumInitialized=!0),this.visualizationsInitialized||(this.parameterLandscapeRef?.nativeElement&&this.initParameterLandscape(),this.trajectoryVisualizationRef?.nativeElement&&this.initTrajectoryVisualization(),this.policyOutputRef?.nativeElement&&this.initPolicyOutput(),this.pendulumInitialized&&this.parameterLandscapeRef?.nativeElement&&this.trajectoryVisualizationRef?.nativeElement&&this.policyOutputRef?.nativeElement&&(this.visualizationsInitialized=!0,this.renderPendulum(),this.resetSimulation()))}catch(a){console.error("Error initializing visualizations:",a)}}toggleSimulation(){this.isSimulationRunning=!this.isSimulationRunning,this.isSimulationRunning&&this.runSimulation()}toggleAutoPlay(){this.isAutoPlay=!this.isAutoPlay}resetSimulation(){this.isSimulationRunning=!1,this.episodeCount=0,this.totalReward=0,this.currentStep=0,this.trajectories=[],this.currentTrajectory=null,this.parameterHistory=[],this.policy=new N(2,1),this.pendulum&&this.resetPendulum(),this.paramSvg&&this.updateParameterLandscape(),this.trajectorySvg&&this.updateTrajectoryVisualization(),this.policySvg&&this.updatePolicyOutput()}updateLearningRate(a){this.learningRate=parseFloat(a.target.value)}updateDiscountFactor(a){this.discountFactor=parseFloat(a.target.value)}runSimulation(){this.isSimulationRunning&&this.ngZone.runOutsideAngular(()=>{let a=()=>{this.isSimulationRunning&&((!this.currentTrajectory||this.currentTrajectory.isComplete)&&this.startNewEpisode(),this.performStep(),this.updatePendulum(),this.updateParameterLandscape(),this.updateTrajectoryVisualization(),this.updatePolicyOutput(),this.currentTrajectory&&this.currentTrajectory.isComplete&&this.isAutoPlay?(this.trainPolicy(),setTimeout(()=>{this.isSimulationRunning&&(this.animationId=requestAnimationFrame(a))},500)):this.currentTrajectory&&!this.currentTrajectory.isComplete?this.animationId=requestAnimationFrame(a):(this.isSimulationRunning=!1,this.ngZone.run(()=>{})))};this.animationId=requestAnimationFrame(a)})}startNewEpisode(){this.currentTrajectory&&this.currentTrajectory.isComplete&&this.trainPolicy(),this.currentTrajectory=new Y,this.trajectories.push(this.currentTrajectory),this.episodeCount++,this.resetPendulum()}performStep(){if(!this.currentTrajectory)return;let a=this.pendulum.rotation.z,o=this.pendulum.userData.angularVelocity,i=[Math.cos(a),Math.sin(a),o],l=this.policy.forward(i).sample();this.pendulum.userData.torque=l*2,this.updatePendulumPhysics();let p=Math.cos(a)-.1*Math.abs(o)-.001*Math.abs(l);this.currentTrajectory.addStep(i,l,p),this.totalReward+=p,this.currentStep++,this.currentTrajectory.steps.length>=200&&this.currentTrajectory.complete()}trainPolicy(){if(!this.currentTrajectory||!this.currentTrajectory.isComplete)return;let a=this.calculateReturns(this.currentTrajectory.steps.map(i=>i.reward));for(let i=0;i<this.currentTrajectory.steps.length;i++){let r=this.currentTrajectory.steps[i];this.policy.update(r.state,r.action,a[i],this.learningRate)}let o=this.policy.getParameters();this.parameterHistory.push({x:o[0],y:o[1],reward:this.currentTrajectory.getTotalReward()})}calculateReturns(a){let o=new Array(a.length).fill(0),i=0;for(let r=a.length-1;r>=0;r--)i=a[r]+this.discountFactor*i,o[r]=i;return o}initPendulumScene(){if(!this.pendulumCanvasRef||!this.pendulumCanvasRef.nativeElement){console.error("Pendulum canvas reference is not available");return}this.scene=new ut,this.scene.background=new dt("#0c1428"),this.camera=new pt(75,1,.1,1e3),this.camera.position.z=5,this.renderer=new ft({canvas:this.pendulumCanvasRef.nativeElement,antialias:!0}),this.renderer.setPixelRatio(window.devicePixelRatio),this.renderer.setSize(400,400),this.controls=new Pt(this.camera,this.renderer.domElement),this.controls.enablePan=!1,this.controls.enableZoom=!1,this.controls.enableRotate=!1;let a=new vt(16777215,.5);this.scene.add(a);let o=new ht(16777215,1);o.position.set(5,5,5),this.scene.add(o),this.createPendulum();let i=new xt(3);this.scene.add(i)}createPendulum(){this.pendulum=new mt;let a=new Z(.1,16,16),o=new F({color:"#4285f4"}),i=new B(a,o);this.pendulum.add(i);let r=new gt(.05,.05,2,16),l=new F({color:"#8bb4fa"}),m=new B(r,l);m.position.y=-1,this.pendulum.add(m);let p=new Z(.2,32,32),g=new F({color:"#ff9d45"}),c=new B(p,g);c.position.y=-2,this.pendulum.add(c),this.scene.add(this.pendulum),this.pendulum.userData={angularVelocity:0,torque:0},this.resetPendulum()}resetPendulum(){this.pendulum&&(this.pendulum.rotation.z=Math.PI+(Math.random()-.5)*.5,this.pendulum.userData.angularVelocity=0,this.pendulum.userData.torque=0)}updatePendulumPhysics(){let l=this.pendulum.rotation.z,p=-(1*9.8*2*Math.sin(l))+this.pendulum.userData.torque;this.pendulum.userData.angularVelocity+=p*.01,this.pendulum.userData.angularVelocity*=1-.1*.01,this.pendulum.rotation.z+=this.pendulum.userData.angularVelocity*.01}renderPendulum(){this.renderer&&this.scene&&this.camera&&this.renderer.render(this.scene,this.camera)}updatePendulum(){this.pendulum&&this.renderPendulum()}initParameterLandscape(){if(!this.parameterLandscapeRef||!this.parameterLandscapeRef.nativeElement){console.error("Parameter landscape reference is not available");return}let a=this.parameterLandscapeRef.nativeElement,o=a.clientWidth,i=a.clientHeight;this.paramSvg=H(a).append("svg").attr("width",o).attr("height",i),this.paramSvg.append("text").attr("x",o/2).attr("y",20).attr("text-anchor","middle").attr("class","visualization-title").text("Policy Parameter Landscape"),this.paramSvg.append("g").attr("class","x-axis").attr("transform",`translate(0, ${i-30})`),this.paramSvg.append("g").attr("class","y-axis").attr("transform","translate(40, 0)"),this.paramSvg.append("g").attr("class","points"),this.updateParameterLandscape()}updateParameterLandscape(){if(!this.paramSvg)return;let a=this.parameterLandscapeRef.nativeElement,o=a.clientWidth,i=a.clientHeight||280;if(this.paramSvg.attr("width",o).attr("height",i),this.parameterHistory.length===0)return;let r=J(this.parameterHistory,h=>h.x),l=r[0]!==void 0?r[0]:0,m=r[1]!==void 0?r[1]:1,p=.5,g=w().domain([l-p,m+p]).range([40,o-20]),c=J(this.parameterHistory,h=>h.y),M=c[0]!==void 0?c[0]:0,y=c[1]!==void 0?c[1]:1,d=w().domain([M-p,y+p]).range([i-30,30]);this.paramSvg.select(".x-axis").attr("transform",`translate(0, ${i-30})`).call(A(g)),this.paramSvg.select(".y-axis").call(G(d));let b=this.parameterHistory.map(h=>h.reward),T=Math.min(...b,0),v=Math.max(...b,1),S=st(ct).domain([T,v]);this.paramSvg.select(".points").selectAll("*").remove(),this.paramSvg.select(".points").selectAll("circle").data(this.parameterHistory).enter().append("circle").attr("r",5).attr("cx",h=>g(h.x)).attr("cy",h=>d(h.y)).attr("fill",h=>S(h.reward)),this.paramSvg.select(".param-path").remove();let _t=D().x(h=>g(h.x)).y(h=>d(h.y));this.parameterHistory.length>1&&this.paramSvg.append("path").attr("class","param-path").datum(this.parameterHistory).attr("fill","none").attr("stroke","#e1e7f5").attr("stroke-width",1.5).attr("d",_t)}initTrajectoryVisualization(){if(!this.trajectoryVisualizationRef||!this.trajectoryVisualizationRef.nativeElement){console.error("Trajectory visualization reference is not available");return}let a=this.trajectoryVisualizationRef.nativeElement,o=a.clientWidth,i=a.clientHeight;this.trajectorySvg=H(a).append("svg").attr("width",o).attr("height",i),this.trajectorySvg.append("text").attr("x",o/2).attr("y",20).attr("text-anchor","middle").attr("class","visualization-title").text("Episode Rewards"),this.trajectorySvg.append("g").attr("class","x-axis").attr("transform",`translate(0, ${i-30})`),this.trajectorySvg.append("g").attr("class","y-axis").attr("transform","translate(40, 0)"),this.updateTrajectoryVisualization()}updateTrajectoryVisualization(){if(!this.trajectorySvg)return;let a=this.trajectoryVisualizationRef.nativeElement,o=a.clientWidth,i=a.clientHeight;if(this.trajectorySvg.attr("width",o).attr("height",i),this.trajectories.length===0)return;let r=this.trajectories.map(d=>d.getTotalReward()),l=w().domain([0,Math.max(10,r.length-1)]).range([40,o-20]),m=r.length>0?Math.min(...r):0,p=r.length>0?Math.max(...r):10,g=(p-m)*.1||10,c=w().domain([m-g,p+g]).range([i-30,30]);this.trajectorySvg.select(".x-axis").call(A(l).ticks(Math.min(10,r.length))),this.trajectorySvg.select(".y-axis").call(G(c)),this.trajectorySvg.select(".reward-path").remove();let M=D().x((d,b)=>l(b)).y(d=>c(d));r.length>1&&this.trajectorySvg.append("path").attr("class","reward-path").datum(r).attr("fill","none").attr("stroke","#ff9d45").attr("stroke-width",2).attr("d",M);let y=this.trajectorySvg.selectAll(".reward-point").data(r);y.enter().append("circle").attr("class","reward-point").attr("r",4).merge(y).attr("cx",(d,b)=>l(b)).attr("cy",d=>c(d)).attr("fill","#ff9d45"),y.exit().remove()}initPolicyOutput(){if(!this.policyOutputRef||!this.policyOutputRef.nativeElement){console.error("Policy output reference is not available");return}let a=this.policyOutputRef.nativeElement,o=a.clientWidth,i=a.clientHeight;this.policySvg=H(a).append("svg").attr("width",o).attr("height",i),this.policySvg.append("text").attr("x",o/2).attr("y",20).attr("text-anchor","middle").attr("class","visualization-title").text("Policy Output Distribution"),this.policySvg.append("g").attr("class","distribution-plot").attr("transform",`translate(${o/2}, ${i/2})`),this.updatePolicyOutput()}updatePolicyOutput(){if(!this.policySvg)return;let a=this.policyOutputRef.nativeElement,o=a.clientWidth,i=a.clientHeight||280;this.policySvg.attr("width",o).attr("height",i);let r=[1,0,0];if(this.pendulum){let v=this.pendulum.rotation.z,S=this.pendulum.userData?.angularVelocity||0;r=[Math.cos(v),Math.sin(v),S]}let l=this.policy.forward(r),m=Array.from({length:101},(v,S)=>-2+S*.04),p=m.map(v=>l.pdf(v)),g=Math.max(...p,.1),c=w().domain([-2,2]).range([50,o-50]),M=w().domain([0,g*1.1]).range([i-50,50]),y=D().x(v=>c(v[0])).y(v=>M(v[1]));this.policySvg.select(".distribution-plot").selectAll("*").remove();let d=this.policySvg.select(".distribution-plot");d.append("g").attr("transform",`translate(0, ${i-50})`).call(A(c)),d.append("g").attr("transform","translate(50, 0)").call(G(M)),d.append("text").attr("x",o/2).attr("y",i-10).attr("text-anchor","middle").attr("class","axis-label").text("Action (Torque)"),d.append("text").attr("transform","rotate(-90)").attr("x",-i/2).attr("y",15).attr("text-anchor","middle").attr("class","axis-label").text("Probability Density");let b=m.map((v,S)=>[v,p[S]]);d.append("path").datum(b).attr("fill","none").attr("stroke","#ff9d45").attr("stroke-width",2).attr("d",y);let T=l.mean;d.append("line").attr("x1",c(T)).attr("y1",M(0)).attr("x2",c(T)).attr("y2",M(g*.8)).attr("stroke","#ffffff").attr("stroke-width",2).attr("stroke-dasharray","5,5"),d.append("text").attr("x",c(T)).attr("y",M(g*.85)).attr("dy",-10).attr("text-anchor","middle").attr("class","mean-label").text(`Mean: ${T.toFixed(2)}`)}resizeVisualizations(){if(this.visualizationsInitialized){if(this.renderer){let a=this.pendulumCanvasRef.nativeElement,o=a.clientWidth,i=a.clientHeight;this.camera.aspect=o/i,this.camera.updateProjectionMatrix(),this.renderer.setSize(o,i),this.renderPendulum()}this.paramSvg&&this.updateParameterLandscape(),this.trajectorySvg&&this.updateTrajectoryVisualization(),this.policySvg&&this.updatePolicyOutput()}}onResize(){this.activeTab==="simulation"&&this.resizeVisualizations()}static \u0275fac=function(o){return new(o||u)(W(K),W(at))};static \u0275cmp=et({type:u,selectors:[["app-policy-gradient"]],viewQuery:function(o,i){if(o&1&&(k(Mt,5),k(bt,5),k(Ot,5),k(yt,5)),o&2){let r;j(r=L())&&(i.pendulumCanvasRef=r.first),j(r=L())&&(i.parameterLandscapeRef=r.first),j(r=L())&&(i.trajectoryVisualizationRef=r.first),j(r=L())&&(i.policyOutputRef=r.first)}},decls:451,vars:37,consts:[["pendulumCanvas",""],["parameterLandscape",""],["trajectoryVisualization",""],["policyOutput",""],[1,"policy-gradient-container",3,"resize"],[1,"header"],[1,"category-badge"],[1,"navigation"],[1,"tabs"],[3,"click"],[1,"content-container"],[1,"tab-content",3,"ngClass"],[1,"overview-content"],[1,"section"],[1,"advantages"],[1,"advantage-card"],[1,"advantage-icon"],["viewBox","0 0 24 24","width","48","height","48"],["cx","12","cy","12","r","10","fill","none","stroke","#ff9d45","stroke-width","2"],["d","M9 12 L11 14 L15 10","stroke","#ff9d45","stroke-width","2","fill","none"],["d","M12 6 L12 18 M6 12 L18 12","stroke","#ff9d45","stroke-width","2"],["d","M8 12 L12 8 L16 12 L12 16 Z","stroke","#ff9d45","stroke-width","2","fill","none"],[1,"concept-grid"],[1,"concept-card"],[1,"section","simulation-preview"],[1,"primary-button",3,"click"],[1,"simulation-content"],[1,"control-panel"],[1,"control-group"],[1,"control-button",3,"click"],[1,"toggle-container"],["for","autoplay"],[1,"toggle-switch",3,"click"],[1,"toggle-slider"],[1,"slider-control"],["for","learning-rate"],["type","range","id","learning-rate","min","0.001","max","0.1","step","0.001",3,"input","value"],["for","discount-factor"],["type","range","id","discount-factor","min","0.8","max","0.999","step","0.001",3,"input","value"],[1,"stats"],[1,"stat-item"],[1,"stat-label"],[1,"stat-value"],[1,"visualization-panel"],[1,"visualization-card","pendulum-viz"],[1,"viz-description"],[1,"visualization-card","parameter-viz"],[1,"viz-container"],[1,"visualization-card","trajectory-viz"],[1,"visualization-card","policy-viz"],[1,"details-content"],[1,"algorithm-box"],[1,"algorithm-step"],[1,"step-number"],[1,"step-content"],[1,"theorem-box"],[1,"math-formula"],[1,"theorem-explanation"],[1,"details-grid"],[1,"detail-card"],[1,"method-list"],[1,"method-item"],[1,"papers-content"],[1,"paper-list"],["class","paper-card",4,"ngFor","ngForOf"],[1,"resource-list"],[1,"resource-card"],[1,"paper-card"],[1,"paper-icon"],["viewBox","0 0 24 24","width","32","height","32"],["d","M14 2H6C4.9 2 4 2.9 4 4V20C4 21.1 4.9 22 6 22H18C19.1 22 20 21.1 20 20V8L14 2Z","fill","none","stroke","#ff9d45","stroke-width","2"],["d","M14 2V8H20","fill","none","stroke","#ff9d45","stroke-width","2"],["d","M8 12H16M8 16H16","stroke","#ff9d45","stroke-width","2"],[1,"paper-details"],[1,"paper-authors"],[1,"paper-citation"],["class","paper-volume",4,"ngIf"],["class","paper-pages",4,"ngIf"],["target","_blank",1,"paper-link",3,"href"],[1,"paper-volume"],[1,"paper-pages"]],template:function(o,i){if(o&1){let r=nt();n(0,"div",4),P("resize",function(){return x(r),f(i.onResize())},!1,tt),n(1,"div",5)(2,"h1"),t(3,"Policy Gradient Methods"),e(),n(4,"div",6),t(5,"Reinforcement Learning"),e()(),n(6,"div",7)(7,"ul",8)(8,"li",9),P("click",function(){return x(r),f(i.selectTab("overview"))}),t(9,"Overview"),e(),n(10,"li",9),P("click",function(){return x(r),f(i.selectTab("simulation"))}),t(11,"Simulation"),e(),n(12,"li",9),P("click",function(){return x(r),f(i.selectTab("details"))}),t(13,"Technical Details"),e(),n(14,"li",9),P("click",function(){return x(r),f(i.selectTab("papers"))}),t(15,"Research Papers"),e()()(),n(16,"div",10)(17,"div",11)(18,"div",12)(19,"div",13)(20,"h2"),t(21,"What are Policy Gradient Methods?"),e(),n(22,"p"),t(23," Policy Gradient (PG) methods are a class of "),n(24,"strong"),t(25,"reinforcement learning"),e(),t(26," algorithms that directly learn a "),n(27,"strong"),t(28,"parameterized policy"),e(),t(29," which maps states to actions (or action probabilities), without necessarily needing to learn a value function first (though many advanced methods do use value functions). Their primary purpose is to optimize the policy parameters to maximize the "),n(30,"strong"),t(31,"expected cumulative reward"),e(),t(32,". "),e()(),n(33,"div",13)(34,"h2"),t(35,"Key Advantages"),e(),n(36,"div",14)(37,"div",15)(38,"div",16),z(),n(39,"svg",17),_(40,"circle",18)(41,"path",19),e()(),R(),n(42,"h3"),t(43,"Continuous Action Spaces"),e(),n(44,"p"),t(45," Naturally applicable to problems where actions are continuous (e.g., robot joint torques, throttle control), which is difficult for purely value-based methods like Q-learning. "),e()(),n(46,"div",15)(47,"div",16),z(),n(48,"svg",17),_(49,"circle",18)(50,"path",20),e()(),R(),n(51,"h3"),t(52,"Stochastic Policies"),e(),n(53,"p"),t(54," Can directly learn policies that output probabilities over actions, which is optimal in partially observable environments or when exploration needs to be inherent in the policy. "),e()(),n(55,"div",15)(56,"div",16),z(),n(57,"svg",17),_(58,"circle",18)(59,"path",21),e()(),R(),n(60,"h3"),t(61,"Better Convergence"),e(),n(62,"p"),t(63," Can sometimes offer smoother convergence properties compared to value-based methods, although they typically converge to a local optimum. "),e()()()(),n(64,"div",13)(65,"h2"),t(66,"Core Concepts"),e(),n(67,"div",22)(68,"div",23)(69,"h3"),t(70,"Parameterized Policy (\u03C0"),n(71,"sub"),t(72,"\u03B8"),e(),t(73,")"),e(),n(74,"p"),t(75," The core idea is to represent the policy as a function with tunable parameters \u03B8 (e.g., the weights of a neural network). This function \u03C0"),n(76,"sub"),t(77,"\u03B8"),e(),t(78,"(a|s) outputs the probability of taking action a in state s. "),e()(),n(79,"div",23)(80,"h3"),t(81,"Objective Function (J(\u03B8))"),e(),n(82,"p"),t(83," The goal is to find the parameters \u03B8 that maximize an objective function J(\u03B8), typically defined as the expected total discounted reward obtained by following the policy \u03C0"),n(84,"sub"),t(85,"\u03B8"),e(),t(86,". "),e()(),n(87,"div",23)(88,"h3"),t(89,"Gradient Ascent"),e(),n(90,"p"),t(91," Policy Gradient methods update the policy parameters \u03B8 by performing gradient ascent on the objective function J(\u03B8): \u03B8 \u2190 \u03B8 + \u03B1\u2207"),n(92,"sub"),t(93,"\u03B8"),e(),t(94,"J(\u03B8), where \u03B1 is the learning rate. "),e()(),n(95,"div",23)(96,"h3"),t(97,"Policy Gradient Theorem"),e(),n(98,"p"),t(99," Provides a way to calculate the gradient \u2207"),n(100,"sub"),t(101,"\u03B8"),e(),t(102,"J(\u03B8) without needing to know the environment's dynamics, making these methods model-free. "),e()()()(),n(103,"div",24)(104,"h2"),t(105,"Simulation Preview"),e(),n(106,"p"),t(107," In our interactive simulation, you'll see Policy Gradient methods applied to control a pendulum system. The algorithm learns a policy that determines the torque to apply, with the goal of balancing the pendulum upright. "),e(),n(108,"button",25),P("click",function(){return x(r),f(i.selectTab("simulation"))}),t(109," Launch Simulation "),e()()()(),n(110,"div",11)(111,"div",26)(112,"div",27)(113,"h2"),t(114,"Simulation Controls"),e(),n(115,"div",28)(116,"button",29),P("click",function(){return x(r),f(i.toggleSimulation())}),t(117),e(),n(118,"button",29),P("click",function(){return x(r),f(i.resetSimulation())}),t(119," Reset "),e(),n(120,"div",30)(121,"label",31),t(122,"Auto Play"),e(),n(123,"div",32),P("click",function(){return x(r),f(i.toggleAutoPlay())}),_(124,"div",33),e()()(),n(125,"div",34)(126,"label",35),t(127),e(),n(128,"input",36),P("input",function(m){return x(r),f(i.updateLearningRate(m))}),e()(),n(129,"div",34)(130,"label",37),t(131),e(),n(132,"input",38),P("input",function(m){return x(r),f(i.updateDiscountFactor(m))}),e()(),n(133,"div",39)(134,"div",40)(135,"div",41),t(136,"Episodes"),e(),n(137,"div",42),t(138),e()(),n(139,"div",40)(140,"div",41),t(141,"Total Reward"),e(),n(142,"div",42),t(143),e()(),n(144,"div",40)(145,"div",41),t(146,"Current Step"),e(),n(147,"div",42),t(148),e()()()(),n(149,"div",43)(150,"div",44)(151,"h3"),t(152,"Pendulum Control"),e(),_(153,"canvas",null,0),n(155,"div",45)(156,"p"),t(157," The 3D pendulum visualization shows the current state of the environment. The policy learns to control the torque applied to the pendulum to keep it balanced upright. "),e()()(),n(158,"div",46)(159,"h3"),t(160,"Policy Parameter Space"),e(),_(161,"div",47,1),n(163,"div",45)(164,"p"),t(165," This visualization shows how the policy parameters change during training. Each point represents the policy after an episode, with color indicating the total reward. "),e()()(),n(166,"div",48)(167,"h3"),t(168,"Learning Progress"),e(),_(169,"div",47,2),n(171,"div",45)(172,"p"),t(173," This graph shows the total reward per episode, illustrating how the policy improves over time as it learns to balance the pendulum more effectively. "),e()()(),n(174,"div",49)(175,"h3"),t(176,"Policy Output"),e(),_(177,"div",47,3),n(179,"div",45)(180,"p"),t(181," This visualization shows the current policy output: a probability distribution over possible actions (torques). The mean of this distribution is the most likely action to take in the current state. "),e()()()()()(),n(182,"div",11)(183,"div",50)(184,"div",13)(185,"h2"),t(186,"Policy Gradient Algorithm"),e(),n(187,"div",51)(188,"h3"),t(189,"REINFORCE Algorithm"),e(),n(190,"div",52)(191,"div",53),t(192,"1"),e(),n(193,"div",54)(194,"strong"),t(195,"Initialization:"),e(),t(196," Initialize the policy parameters \u03B8 randomly "),e()(),n(197,"div",52)(198,"div",53),t(199,"2"),e(),n(200,"div",54)(201,"strong"),t(202,"Generate Trajectories:"),e(),t(203," Execute the current policy \u03C0"),n(204,"sub"),t(205,"\u03B8"),e(),t(206," in the environment to collect complete trajectories "),e()(),n(207,"div",52)(208,"div",53),t(209,"3"),e(),n(210,"div",54)(211,"strong"),t(212,"Calculate Returns:"),e(),t(213," For each time step t, calculate the discounted return-to-go: G"),n(214,"sub"),t(215,"t"),e(),t(216," = \u03A3"),n(217,"sub"),t(218,"k=t"),e(),n(219,"sup"),t(220,"T"),e(),t(221," \u03B3"),n(222,"sup"),t(223,"k-t"),e(),t(224,"r"),n(225,"sub"),t(226,"k+1"),e()()(),n(227,"div",52)(228,"div",53),t(229,"4"),e(),n(230,"div",54)(231,"strong"),t(232,"Estimate Policy Gradient:"),e(),t(233," Compute the gradient estimate using the Policy Gradient Theorem "),e()(),n(234,"div",52)(235,"div",53),t(236,"5"),e(),n(237,"div",54)(238,"strong"),t(239,"Update Parameters:"),e(),t(240," \u03B8 \u2190 \u03B8 + \u03B1\u2207"),n(241,"sub"),t(242,"\u03B8"),e(),t(243,"J(\u03B8) "),e()(),n(244,"div",52)(245,"div",53),t(246,"6"),e(),n(247,"div",54)(248,"strong"),t(249,"Repeat:"),e(),t(250," Discard the collected trajectories and repeat the process with the updated policy "),e()()()(),n(251,"div",13)(252,"h2"),t(253,"Policy Gradient Theorem"),e(),n(254,"div",55)(255,"p",56),t(256," \u2207"),n(257,"sub"),t(258,"\u03B8"),e(),t(259,"J(\u03B8) = E"),n(260,"sub"),t(261,"\u03C4 ~ \u03C0"),n(262,"sub"),t(263,"\u03B8"),e()(),t(264," [\u03A3"),n(265,"sub"),t(266,"t=0"),e(),n(267,"sup"),t(268,"T"),e(),t(269," \u2207"),n(270,"sub"),t(271,"\u03B8"),e(),t(272,"log \u03C0"),n(273,"sub"),t(274,"\u03B8"),e(),t(275,"(a"),n(276,"sub"),t(277,"t"),e(),t(278,"|s"),n(279,"sub"),t(280,"t"),e(),t(281,") \xB7 \u03A8"),n(282,"sub"),t(283,"t"),e(),t(284,"] "),e(),n(285,"div",57)(286,"p")(287,"strong"),t(288,"\u2207"),n(289,"sub"),t(290,"\u03B8"),e(),t(291,"log \u03C0"),n(292,"sub"),t(293,"\u03B8"),e(),t(294,"(a"),n(295,"sub"),t(296,"t"),e(),t(297,"|s"),n(298,"sub"),t(299,"t"),e(),t(300,"):"),e(),t(301," This term is the "),n(302,"em"),t(303,"score function"),e(),t(304,". It indicates how sensitive the log-probability of taking action a"),n(305,"sub"),t(306,"t"),e(),t(307," in state s"),n(308,"sub"),t(309,"t"),e(),t(310," is to changes in the parameters \u03B8. "),e(),n(311,"p")(312,"strong"),t(313,"\u03A8"),n(314,"sub"),t(315,"t"),e(),t(316,":"),e(),t(317,' This term is a measure of how "good" the action a'),n(318,"sub"),t(319,"t"),e(),t(320," taken at time step t was. It weights the score function. Different algorithms use different choices for \u03A8"),n(321,"sub"),t(322,"t"),e(),t(323,": "),e(),n(324,"ul")(325,"li")(326,"strong"),t(327,"REINFORCE Algorithm:"),e(),t(328," Uses the total discounted return from time step t onwards: \u03A8"),n(329,"sub"),t(330,"t"),e(),t(331," = G"),n(332,"sub"),t(333,"t"),e()(),n(334,"li")(335,"strong"),t(336,"Baselines:"),e(),t(337," To reduce variance, a state-dependent baseline b(s"),n(338,"sub"),t(339,"t"),e(),t(340,") is subtracted: \u03A8"),n(341,"sub"),t(342,"t"),e(),t(343," = G"),n(344,"sub"),t(345,"t"),e(),t(346," - b(s"),n(347,"sub"),t(348,"t"),e(),t(349,") "),e(),n(350,"li")(351,"strong"),t(352,"Actor-Critic Methods:"),e(),t(353," Use an estimate of the Advantage function A(s"),n(354,"sub"),t(355,"t"),e(),t(356,", a"),n(357,"sub"),t(358,"t"),e(),t(359,") = Q(s"),n(360,"sub"),t(361,"t"),e(),t(362,", a"),n(363,"sub"),t(364,"t"),e(),t(365,") - V(s"),n(366,"sub"),t(367,"t"),e(),t(368,") for \u03A8"),n(369,"sub"),t(370,"t"),e()()()()()(),n(371,"div",13)(372,"h2"),t(373,"Key Details and Practical Considerations"),e(),n(374,"div",58)(375,"div",59)(376,"h3"),t(377,"High Variance"),e(),n(378,"p"),t(379," The gradient estimates in basic PG methods often have high variance because the return depends on many random actions and state transitions. This can make learning slow and unstable. Techniques like baselines and actor-critic methods are crucial for practical applications. "),e()(),n(380,"div",59)(381,"h3"),t(382,"On-Policy Learning"),e(),n(383,"p"),t(384," Most simple PG algorithms are "),n(385,"strong"),t(386,"on-policy"),e(),t(387,", meaning the trajectories used to compute the gradient must be generated by the current version of the policy \u03C0"),n(388,"sub"),t(389,"\u03B8"),e(),t(390,". This can be sample-inefficient as data is often discarded after one update. "),e()(),n(391,"div",59)(392,"h3"),t(393,"Local Optima"),e(),n(394,"p"),t(395," Gradient ascent guarantees convergence only to a local optimum of the objective function J(\u03B8). The policy might get stuck in suboptimal behaviors depending on initialization and environment dynamics. "),e()(),n(396,"div",59)(397,"h3"),t(398,"Hyperparameter Sensitivity"),e(),n(399,"p"),t(400," Performance is sensitive to the learning rate \u03B1 and potentially other parameters like the neural network architecture or policy representation. Careful tuning is often necessary for good results. "),e()()()(),n(401,"div",13)(402,"h2"),t(403,"Advanced Policy Gradient Methods"),e(),n(404,"div",60)(405,"div",61)(406,"h3"),t(407,"Actor-Critic"),e(),n(408,"p"),t(409," Combines policy gradient (actor) with value function estimation (critic) to reduce variance in updates. "),e()(),n(410,"div",61)(411,"h3"),t(412,"Trust Region Policy Optimization (TRPO)"),e(),n(413,"p"),t(414," Constrains policy updates to ensure stability by limiting the KL divergence between old and new policies. "),e()(),n(415,"div",61)(416,"h3"),t(417,"Proximal Policy Optimization (PPO)"),e(),n(418,"p"),t(419," A simplified version of TRPO that uses a clipped objective function to achieve similar performance with better computational efficiency. "),e()(),n(420,"div",61)(421,"h3"),t(422,"Deterministic Policy Gradient (DPG)"),e(),n(423,"p"),t(424," Learns deterministic policies (rather than stochastic) which can be more efficient in certain continuous control tasks. "),e()()()()()(),n(425,"div",11)(426,"div",62)(427,"div",13)(428,"h2"),t(429,"Foundational Research Papers"),e(),n(430,"div",63),Q(431,wt,17,7,"div",64),e()(),n(432,"div",13)(433,"h2"),t(434,"Additional Resources"),e(),n(435,"div",65)(436,"div",66)(437,"h3"),t(438,"Reinforcement Learning: An Introduction"),e(),n(439,"p"),t(440," By Richard S. Sutton and Andrew G. Barto (2018, 2nd Edition). This textbook provides a comprehensive introduction to reinforcement learning, including a detailed discussion of policy gradient methods in Chapter 13. "),e()(),n(441,"div",66)(442,"h3"),t(443,"Policy Gradient Algorithms"),e(),n(444,"p"),t(445," By Lilian Weng (2018). A blog post providing an overview of various policy gradient algorithms, their similarities, differences, and evolutionary relationships. "),e()(),n(446,"div",66)(447,"h3"),t(448,"Spinning Up in Deep RL"),e(),n(449,"p"),t(450," By OpenAI. A comprehensive educational resource that includes clear explanations of various policy gradient algorithms and their implementations. "),e()()()()()()()()}o&2&&(s(8),E("active",i.activeTab==="overview"),s(2),E("active",i.activeTab==="simulation"),s(2),E("active",i.activeTab==="details"),s(2),E("active",i.activeTab==="papers"),s(3),C("ngClass",V(25,q,i.activeTab==="overview",i.activeTab!=="overview")),s(93),C("ngClass",V(28,q,i.activeTab==="simulation",i.activeTab!=="simulation")),s(6),E("active",i.isSimulationRunning),s(),I(" ",i.isSimulationRunning?"Pause":"Start"," "),s(6),E("active",i.isAutoPlay),s(4),I("Learning Rate: ",i.learningRate,""),s(),C("value",i.learningRate),s(3),I("Discount Factor: ",i.discountFactor,""),s(),C("value",i.discountFactor),s(6),O(i.episodeCount),s(5),O(i.totalReward.toFixed(1)),s(5),O(i.currentStep),s(34),C("ngClass",V(31,q,i.activeTab==="details",i.activeTab!=="details")),s(243),C("ngClass",V(34,q,i.activeTab==="papers",i.activeTab!=="papers")),s(6),C("ngForOf",i.papers))},dependencies:[lt,rt,ot],styles:['*[_ngcontent-%COMP%]{box-sizing:border-box;margin:0;padding:0}.policy-gradient-container[_ngcontent-%COMP%]{font-family:Inter,Roboto,sans-serif;color:#e1e7f5;background-color:#0c1428;min-height:100vh;width:100%;overflow-x:hidden}.header[_ngcontent-%COMP%]{padding:24px 32px;background-color:#162a4a;box-shadow:0 4px 8px #00000026;display:flex;justify-content:space-between;align-items:center}.header[_ngcontent-%COMP%]   h1[_ngcontent-%COMP%]{font-size:28px;font-weight:600;color:#fff}.header[_ngcontent-%COMP%]   .category-badge[_ngcontent-%COMP%]{background-color:#ff9d45;color:#fff;padding:4px 16px;border-radius:8px;font-weight:500;font-size:14px;letter-spacing:.5px}.navigation[_ngcontent-%COMP%]{background-color:#1e3a66;padding:0 32px;box-shadow:0 2px 4px #0000001a}.navigation[_ngcontent-%COMP%]   .tabs[_ngcontent-%COMP%]{display:flex;list-style:none}.navigation[_ngcontent-%COMP%]   .tabs[_ngcontent-%COMP%]   li[_ngcontent-%COMP%]{padding:16px 24px;cursor:pointer;font-weight:500;border-bottom:3px solid transparent;transition:all .2s ease}.navigation[_ngcontent-%COMP%]   .tabs[_ngcontent-%COMP%]   li[_ngcontent-%COMP%]:hover{background-color:#2a49804d}.navigation[_ngcontent-%COMP%]   .tabs[_ngcontent-%COMP%]   li.active[_ngcontent-%COMP%]{border-bottom-color:#ff9d45;color:#fff}.content-container[_ngcontent-%COMP%]{padding:32px}.tab-content[_ngcontent-%COMP%]{background-color:#162a4a;border-radius:12px;box-shadow:0 4px 8px #00000026;overflow:hidden}.tab-content.hidden[_ngcontent-%COMP%]{display:none}.tab-content.visible[_ngcontent-%COMP%]{display:block}.section[_ngcontent-%COMP%]{margin-bottom:32px}.section[_ngcontent-%COMP%]   h2[_ngcontent-%COMP%]{color:#fff;font-size:22px;margin-bottom:24px;font-weight:600;position:relative}.section[_ngcontent-%COMP%]   h2[_ngcontent-%COMP%]:after{content:"";position:absolute;bottom:-8px;left:0;width:40px;height:3px;background-color:#ff9d45;border-radius:2px}.overview-content[_ngcontent-%COMP%]{padding:32px}.overview-content[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{line-height:1.6;margin-bottom:16px}.overview-content[_ngcontent-%COMP%]   .advantages[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:24px;margin-top:24px}.overview-content[_ngcontent-%COMP%]   .advantages[_ngcontent-%COMP%]   .advantage-card[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px;display:flex;flex-direction:column;align-items:center;text-align:center;transition:transform .2s ease}.overview-content[_ngcontent-%COMP%]   .advantages[_ngcontent-%COMP%]   .advantage-card[_ngcontent-%COMP%]:hover{transform:translateY(-5px)}.overview-content[_ngcontent-%COMP%]   .advantages[_ngcontent-%COMP%]   .advantage-card[_ngcontent-%COMP%]   .advantage-icon[_ngcontent-%COMP%]{margin-bottom:16px}.overview-content[_ngcontent-%COMP%]   .advantages[_ngcontent-%COMP%]   .advantage-card[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;margin-bottom:16px;font-size:18px}.overview-content[_ngcontent-%COMP%]   .advantages[_ngcontent-%COMP%]   .advantage-card[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{color:#8a9ab0;font-size:14px}.overview-content[_ngcontent-%COMP%]   .concept-grid[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:24px;margin-top:24px}.overview-content[_ngcontent-%COMP%]   .concept-grid[_ngcontent-%COMP%]   .concept-card[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px;transition:all .2s ease}.overview-content[_ngcontent-%COMP%]   .concept-grid[_ngcontent-%COMP%]   .concept-card[_ngcontent-%COMP%]:hover{background-color:#2a4980}.overview-content[_ngcontent-%COMP%]   .concept-grid[_ngcontent-%COMP%]   .concept-card[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;margin-bottom:16px;font-size:18px}.overview-content[_ngcontent-%COMP%]   .concept-grid[_ngcontent-%COMP%]   .concept-card[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{color:#8a9ab0;font-size:14px;line-height:1.5}.overview-content[_ngcontent-%COMP%]   .simulation-preview[_ngcontent-%COMP%]{background-color:#1e3a66;padding:32px;border-radius:12px;text-align:center}.overview-content[_ngcontent-%COMP%]   .simulation-preview[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{max-width:800px;margin:0 auto 24px}.overview-content[_ngcontent-%COMP%]   .simulation-preview[_ngcontent-%COMP%]   .primary-button[_ngcontent-%COMP%]{background:linear-gradient(135deg,#ff9d45,#e67e17);color:#fff;border:none;padding:16px 32px;border-radius:8px;font-weight:500;cursor:pointer;transition:all .2s ease}.overview-content[_ngcontent-%COMP%]   .simulation-preview[_ngcontent-%COMP%]   .primary-button[_ngcontent-%COMP%]:hover{transform:translateY(-2px);box-shadow:0 4px 8px #00000026}.simulation-content[_ngcontent-%COMP%]{display:grid;grid-template-columns:300px 1fr;gap:24px;padding:24px;max-height:850px;overflow:hidden}@media (max-width: 1200px){.simulation-content[_ngcontent-%COMP%]{grid-template-columns:1fr;max-height:none}}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   h2[_ngcontent-%COMP%]{font-size:20px;margin-bottom:24px;color:#fff}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .control-group[_ngcontent-%COMP%]{display:flex;flex-wrap:wrap;gap:16px;margin-bottom:24px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .control-button[_ngcontent-%COMP%]{background-color:#2a4980;color:#fff;border:none;padding:16px 24px;border-radius:8px;cursor:pointer;flex:1;min-width:80px;font-weight:500;transition:all .2s ease}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .control-button[_ngcontent-%COMP%]:hover{background-color:#243e6d}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .control-button.active[_ngcontent-%COMP%]{background-color:#ff9d45}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .toggle-container[_ngcontent-%COMP%]{display:flex;align-items:center;gap:16px;margin-top:16px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .toggle-container[_ngcontent-%COMP%]   label[_ngcontent-%COMP%]{font-size:14px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .toggle-container[_ngcontent-%COMP%]   .toggle-switch[_ngcontent-%COMP%]{width:50px;height:24px;background-color:#2a4980;border-radius:12px;position:relative;cursor:pointer}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .toggle-container[_ngcontent-%COMP%]   .toggle-switch[_ngcontent-%COMP%]   .toggle-slider[_ngcontent-%COMP%]{width:20px;height:20px;background-color:#fff;border-radius:50%;position:absolute;top:2px;left:2px;transition:transform .2s ease}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .toggle-container[_ngcontent-%COMP%]   .toggle-switch.active[_ngcontent-%COMP%]{background-color:#ff9d45}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .toggle-container[_ngcontent-%COMP%]   .toggle-switch.active[_ngcontent-%COMP%]   .toggle-slider[_ngcontent-%COMP%]{transform:translate(26px)}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .slider-control[_ngcontent-%COMP%]{margin-bottom:24px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .slider-control[_ngcontent-%COMP%]   label[_ngcontent-%COMP%]{display:block;margin-bottom:8px;font-size:14px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .slider-control[_ngcontent-%COMP%]   input[type=range][_ngcontent-%COMP%]{width:100%;-webkit-appearance:none;height:4px;background:#2a4980;border-radius:2px;outline:none}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .slider-control[_ngcontent-%COMP%]   input[type=range][_ngcontent-%COMP%]::-webkit-slider-thumb{-webkit-appearance:none;width:18px;height:18px;border-radius:50%;background:#ff9d45;cursor:pointer}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .stats[_ngcontent-%COMP%]{background-color:#162a4a;border-radius:8px;padding:16px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .stats[_ngcontent-%COMP%]   .stat-item[_ngcontent-%COMP%]{display:flex;justify-content:space-between;margin-bottom:8px}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .stats[_ngcontent-%COMP%]   .stat-item[_ngcontent-%COMP%]:last-child{margin-bottom:0}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .stats[_ngcontent-%COMP%]   .stat-item[_ngcontent-%COMP%]   .stat-label[_ngcontent-%COMP%]{font-size:14px;color:#8a9ab0}.simulation-content[_ngcontent-%COMP%]   .control-panel[_ngcontent-%COMP%]   .stats[_ngcontent-%COMP%]   .stat-item[_ngcontent-%COMP%]   .stat-value[_ngcontent-%COMP%]{font-weight:500;color:#fff}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(400px,1fr));gap:24px}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]{background-color:#162a4a;border-radius:12px;padding:16px;display:flex;flex-direction:column;height:400px}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{font-size:16px;margin-bottom:16px;color:#fff;text-align:center}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   canvas[_ngcontent-%COMP%], .simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   .viz-container[_ngcontent-%COMP%]{flex:1;height:280px;min-height:250px;max-height:280px;border-radius:8px;overflow:hidden;background-color:#0c1428}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   .viz-description[_ngcontent-%COMP%]{margin-top:16px;height:60px;overflow-y:auto}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   .viz-description[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{font-size:12px;color:#8a9ab0;line-height:1.4}.simulation-content[_ngcontent-%COMP%]   .visualization-panel[_ngcontent-%COMP%]   .visualization-card.pendulum-viz[_ngcontent-%COMP%]   canvas[_ngcontent-%COMP%]{background-color:#0c1428}.details-content[_ngcontent-%COMP%]{padding:32px}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px;margin-bottom:32px}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;font-size:20px;margin-bottom:24px}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]   .algorithm-step[_ngcontent-%COMP%]{display:flex;margin-bottom:16px}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]   .algorithm-step[_ngcontent-%COMP%]:last-child{margin-bottom:0}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]   .algorithm-step[_ngcontent-%COMP%]   .step-number[_ngcontent-%COMP%]{width:30px;height:30px;border-radius:50%;background-color:#ff9d45;color:#fff;display:flex;align-items:center;justify-content:center;font-weight:600;margin-right:16px;flex-shrink:0}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]   .algorithm-step[_ngcontent-%COMP%]   .step-content[_ngcontent-%COMP%]{padding-top:4px}.details-content[_ngcontent-%COMP%]   .algorithm-box[_ngcontent-%COMP%]   .algorithm-step[_ngcontent-%COMP%]   .step-content[_ngcontent-%COMP%]   strong[_ngcontent-%COMP%]{color:#fff}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px;margin-bottom:32px}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]   .math-formula[_ngcontent-%COMP%]{text-align:center;font-size:18px;padding:16px;margin-bottom:24px;color:#fff;background-color:#0c142880;border-radius:8px}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]   .theorem-explanation[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{margin-bottom:16px;line-height:1.6}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]   .theorem-explanation[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]   strong[_ngcontent-%COMP%]{color:#fff}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]   .theorem-explanation[_ngcontent-%COMP%]   ul[_ngcontent-%COMP%]{margin-left:32px}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]   .theorem-explanation[_ngcontent-%COMP%]   ul[_ngcontent-%COMP%]   li[_ngcontent-%COMP%]{margin-bottom:8px;line-height:1.6}.details-content[_ngcontent-%COMP%]   .theorem-box[_ngcontent-%COMP%]   .theorem-explanation[_ngcontent-%COMP%]   ul[_ngcontent-%COMP%]   li[_ngcontent-%COMP%]   strong[_ngcontent-%COMP%]{color:#fff}.details-content[_ngcontent-%COMP%]   .details-grid[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:24px}.details-content[_ngcontent-%COMP%]   .details-grid[_ngcontent-%COMP%]   .detail-card[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px}.details-content[_ngcontent-%COMP%]   .details-grid[_ngcontent-%COMP%]   .detail-card[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;font-size:18px;margin-bottom:16px}.details-content[_ngcontent-%COMP%]   .details-grid[_ngcontent-%COMP%]   .detail-card[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{font-size:14px;line-height:1.6;color:#8a9ab0}.details-content[_ngcontent-%COMP%]   .method-list[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:24px}.details-content[_ngcontent-%COMP%]   .method-list[_ngcontent-%COMP%]   .method-item[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px;border-left:4px solid #ff9d45}.details-content[_ngcontent-%COMP%]   .method-list[_ngcontent-%COMP%]   .method-item[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;font-size:18px;margin-bottom:16px}.details-content[_ngcontent-%COMP%]   .method-list[_ngcontent-%COMP%]   .method-item[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{font-size:14px;line-height:1.6;color:#8a9ab0}.papers-content[_ngcontent-%COMP%]{padding:32px}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]{display:flex;flex-direction:column;gap:24px}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px;display:flex;align-items:flex-start}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-icon[_ngcontent-%COMP%]{margin-right:24px;flex-shrink:0}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;font-size:18px;margin-bottom:8px}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   .paper-authors[_ngcontent-%COMP%]{color:#e1e7f5;font-size:14px;margin-bottom:8px}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   .paper-citation[_ngcontent-%COMP%], .papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   .paper-volume[_ngcontent-%COMP%], .papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   .paper-pages[_ngcontent-%COMP%]{color:#8a9ab0;font-size:14px;margin-bottom:4px}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   .paper-link[_ngcontent-%COMP%]{display:inline-block;margin-top:16px;color:#ff9d45;text-decoration:none;font-weight:500}.papers-content[_ngcontent-%COMP%]   .paper-list[_ngcontent-%COMP%]   .paper-card[_ngcontent-%COMP%]   .paper-details[_ngcontent-%COMP%]   .paper-link[_ngcontent-%COMP%]:hover{text-decoration:underline}.papers-content[_ngcontent-%COMP%]   .resource-list[_ngcontent-%COMP%]{display:grid;grid-template-columns:repeat(auto-fit,minmax(300px,1fr));gap:24px}.papers-content[_ngcontent-%COMP%]   .resource-list[_ngcontent-%COMP%]   .resource-card[_ngcontent-%COMP%]{background-color:#1e3a66;border-radius:12px;padding:24px}.papers-content[_ngcontent-%COMP%]   .resource-list[_ngcontent-%COMP%]   .resource-card[_ngcontent-%COMP%]   h3[_ngcontent-%COMP%]{color:#fff;font-size:18px;margin-bottom:16px}.papers-content[_ngcontent-%COMP%]   .resource-list[_ngcontent-%COMP%]   .resource-card[_ngcontent-%COMP%]   p[_ngcontent-%COMP%]{font-size:14px;line-height:1.6;color:#8a9ab0}.visualization-title[_ngcontent-%COMP%]{font-size:14px;fill:#fff}.x-axis[_ngcontent-%COMP%], .y-axis[_ngcontent-%COMP%]{font-size:10px}.x-axis[_ngcontent-%COMP%]   path[_ngcontent-%COMP%], .x-axis[_ngcontent-%COMP%]   line[_ngcontent-%COMP%], .y-axis[_ngcontent-%COMP%]   path[_ngcontent-%COMP%], .y-axis[_ngcontent-%COMP%]   line[_ngcontent-%COMP%]{stroke:#8a9ab0}.x-axis[_ngcontent-%COMP%]   text[_ngcontent-%COMP%], .y-axis[_ngcontent-%COMP%]   text[_ngcontent-%COMP%]{fill:#8a9ab0}.param-path[_ngcontent-%COMP%], .reward-path[_ngcontent-%COMP%]{stroke-linejoin:round;stroke-linecap:round}.axis-label[_ngcontent-%COMP%]{font-size:12px;fill:#8a9ab0}.mean-label[_ngcontent-%COMP%]{font-size:12px;fill:#fff}@media (max-width: 768px){.header[_ngcontent-%COMP%]{flex-direction:column;align-items:flex-start}.header[_ngcontent-%COMP%]   .category-badge[_ngcontent-%COMP%]{margin-top:8px}.navigation[_ngcontent-%COMP%]   .tabs[_ngcontent-%COMP%]{overflow-x:auto;padding-bottom:4px}.navigation[_ngcontent-%COMP%]   .tabs[_ngcontent-%COMP%]   li[_ngcontent-%COMP%]{padding:16px 8px;font-size:14px;white-space:nowrap}.content-container[_ngcontent-%COMP%]{padding:16px}.simulation-content[_ngcontent-%COMP%], .visualization-panel[_ngcontent-%COMP%]{grid-template-columns:1fr}.visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   canvas[_ngcontent-%COMP%], .visualization-panel[_ngcontent-%COMP%]   .visualization-card[_ngcontent-%COMP%]   .viz-container[_ngcontent-%COMP%]{min-height:200px}}']})},U=class{constructor(a,o){this.mean=a;this.stdDev=o}sample(){let a=Math.random(),o=Math.random(),i=Math.sqrt(-2*Math.log(a))*Math.cos(2*Math.PI*o);return this.mean+this.stdDev*i}pdf(a){let o=this.stdDev*this.stdDev;return 1/Math.sqrt(2*Math.PI*o)*Math.exp(-.5*Math.pow((a-this.mean)/this.stdDev,2))}logProb(a){let o=this.stdDev*this.stdDev;return-.5*Math.log(2*Math.PI*o)-.5*Math.pow((a-this.mean)/this.stdDev,2)}},N=class{weights;bias;constructor(a,o){this.weights=Array.from({length:a},()=>(Math.random()-.5)*.1),this.bias=(Math.random()-.5)*.1}forward(a){let o=this.bias;for(let r=0;r<this.weights.length&&r<a.length;r++)o+=this.weights[r]*a[r];let i=.5;return new U(o,i)}update(a,o,i,r){let l=this.forward(a),m=l.logProb(o),g=(o-l.mean)/(l.stdDev*l.stdDev);for(let c=0;c<this.weights.length&&c<a.length;c++)this.weights[c]+=r*i*g*a[c];this.bias+=r*i*g}getParameters(){return[this.weights[0]||0,this.weights[1]||0]}},Y=class{steps=[];isComplete=!1;addStep(a,o,i){this.steps.push({state:a,action:o,reward:i})}complete(){this.isComplete=!0}getTotalReward(){return this.steps.reduce((a,o)=>a+o.reward,0)}};export{Ct as PolicyGradientComponent};
